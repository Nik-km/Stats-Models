{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using Gradient Descent\n",
    "\n",
    "Have a dataset with $X$ (features) and $y$ (labels) and we want to fit a straight line to it. Gradient Descent is an iterative algorithm meaning that you need to take multiple steps to get to the global optimum (to find the optimal parameters).\n",
    "\n",
    "Notation\n",
    "- $n$ = number of features\n",
    "- $m$ = number of training observations\n",
    "- $X$ = features; input data matrix of size ($m$ x $n$)\n",
    "- $y$ = true/label/target value; matrix of size ($m$ x 1)\n",
    "- $x_i$, $y_i$ = $i$-th observation in the training set\n",
    "- $w$ = weights (parameters); vector of size ($n$ x 1)\n",
    "- $b$ = bias (parameter); scalar (a real number) that can be [broadcasted](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n",
    "- $\\theta$ = the parameter\n",
    "- $\\hat{y}$ = prediction/hypothesis (dot product of theta & X)\n",
    "\n",
    "\n",
    "We can represent the data in 2-dimensions, so we can say that weights are going to be the slope of the line & bias will be the y-intercept. \n",
    "But if we had two features in our data instead of just one, then we would represent the data in 3-dimensions and we would require a plane to fit the data in the 3D space. \n",
    "So, our hypothesis will be a plane rather than a straight line. \n",
    "As the number of features increases, the dimensions of our weights and bias increase.\n",
    "\n",
    "\n",
    "`weights` and `bias` are vectors and the dimensions of $w$ and $b$ equal the number of features (`weights` & `bias` are also called the parameters of the learning algorithm).\n",
    "\n",
    "Goal is to find the values of `weights` & `bias` s.t. $\\hat{y}$ is as close to $y$ as possible.\n",
    "    $$\\hat{y} = wX + b$$\n",
    "\n",
    "\n",
    "**Vectorized prediction/hypothesis**\n",
    "    $$\\hat{y} = \\theta^T \\cdot X$$\n",
    "\n",
    "**Loss Function**\n",
    "\n",
    "Mean squared error (MSE) loss (y - y_hat)²\n",
    "    $$MSE = \\sum_{i=1}^{m} (y_i - \\theta^T x_i)^2$$\n",
    "\n",
    "- m = the number of training examples\n",
    "- n = number of features\n",
    "\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "<p align=\"center\"><img title=\"Sigmoid function plot\" alt=\"Sigmoid function plot\" src=\"../.attachments/gradient_descent_flowchart.png\"></p>\n",
    "\n",
    "First, we initialize the parameter theta randomly or with all zeros. Then,\n",
    "1) Calculate the prediction/hypothesis $\\hat{y}$ using the following equation\n",
    "    $$\\hat{y} = \\theta^T \\cdot X$$\n",
    "2) Use the prediction/hypothesis $\\hat{y}$ to calculate MSE loss — $(y - \\hat{y})^2$\n",
    "3) Then take the partial derivative (gradient) of the MSE loss with respect to the parameter theta\n",
    "4) Finally use this partial derivative (gradient) to update the parameter theta (where `lr` = learning rate)\n",
    "    $$\\theta := \\theta - lr * gradient$$\n",
    "5) Repeat steps 1-4 until we reach an optimal value for the parameter theta\n",
    "\n",
    "Gradient Descent Process of doing a Linear Regression\n",
    "To calculate theta, we take the partial derivative of the MSE loss function with respect to theta and set it equal to zero. More formally, this process would be:\n",
    "    $$\\sum_{i=1}^{m} (y_i - \\theta^T x_i)^2$$\n",
    "\n",
    "Then, do a little bit of linear algebra to get the value of theta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(500,1)                  # (rows, columns)\n",
    "y = 2*X + 1 + 1.2*np.random.randn(500,1)    # vector of length 500\n",
    "\n",
    "\n",
    "y_hat = np.dot(X, weights) + bias"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
